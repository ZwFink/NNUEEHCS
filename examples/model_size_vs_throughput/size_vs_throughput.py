#!/usr/bin/env python3
"""
Experiment script to measure uncertainty estimation metrics for different UQ methods.

This script evaluates how different UQ techniques and their hyperparameters affect 
various metrics of a base model architecture. It is designed to be run with parameter
configurations generated by generate_parameter_grid.py, enabling parallel execution
across parameter spaces.

Usage:
    # First generate parameter configurations
    python generate_parameter_grid.py --config config.yaml --num-samples 100 --output parameter_configs.json
    
    # Then run experiments with specific configuration IDs
    python size_vs_throughput.py --config_id no_uq_000 --param_config parameter_configs.json
    python size_vs_throughput.py --config_id ensemble_001 --param_config parameter_configs.json
    python size_vs_throughput.py --config_id delta_uq_042 --param_config parameter_configs.json
    
    # Use different model architectures
    python size_vs_throughput.py --config_id no_uq_000 --param_config parameter_configs.json --model_name resnet_50_size
    python size_vs_throughput.py --config_id no_uq_000 --param_config parameter_configs.json --model_name resnet_101_size
"""

import torch
from torch import nn
import click
import sys
import json
import time
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, Any
torch.set_grad_enabled(False)

from nnueehcs.model_builder import get_model_builder_class
from nnueehcs.data_utils import get_dataset
from nnueehcs.evaluation import get_evaluator
import yaml


@dataclass
class ExperimentResult:
    """Results from a single experiment with multiple metrics."""
    uq_method: str
    parameters: Dict[str, Any]
    total_params: int
    trainable_params: int
    model_size_mb: float
    metric_results: Dict[str, float]
    experiment_timestamp: str
    base_architecture: str
    dataset_name: str


def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from YAML file."""
    try:
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"Error: Config file {config_path} not found")
        sys.exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing config file: {e}")
        sys.exit(1)


def load_parameter_config(param_config_path: str, config_id: str) -> tuple[str, Dict[str, Any]]:
    """Load UQ method and parameters from parameter configuration file."""
    try:
        with open(param_config_path, 'r') as f:
            param_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Parameter config file {param_config_path} not found")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"Error parsing parameter config file: {e}")
        sys.exit(1)
    
    # Find the configuration with the specified config_id
    configurations = param_data.get('configurations', [])
    for config in configurations:
        if config.get('config_id') == config_id:
            uq_method = config.get('uq_method')
            parameters = config.get('parameters', {})
            return uq_method, parameters
    
    # If not found, print available config_ids for debugging
    available_ids = [cfg.get('config_id') for cfg in configurations]
    print(f"Error: Config ID '{config_id}' not found in parameter configuration file")
    print(f"Available config IDs: {available_ids[:10]}...")
    sys.exit(1)


def count_model_parameters(model: nn.Module) -> tuple:
    """Count total and trainable parameters in a model."""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def build_model(config: Dict[str, Any], model_name: str, uq_method: str, uq_params: Dict[str, Any]) -> nn.Module:
    """Build model with specified UQ method and parameters."""
    
    if model_name not in config['models']:
        available_models = list(config['models'].keys())
        raise ValueError(f"Model '{model_name}' not found in config. Available models: {available_models}")
    
    model_cfg = config['models'][model_name]
    base_architecture = model_cfg['architecture']
    
    model_builder_class = get_model_builder_class(uq_method)
    return model_builder_class(base_architecture, uq_params, train_config={}).build().to(torch.float64).eval()

def prepare_data(config: Dict[str, Any], dataset_name: str):
    """Prepare evaluation datasets."""
    try:
        dataset = get_dataset(config['dataset'], dataset_name)
        return dataset
        
    except Exception as e:
        print(f"Error preparing data: {e}")
        raise


def fit_model(model: nn.Module, train_dataset):
    """Fit the model to training data."""
    print("Fitting model to training data...")
    model.fit(train_dataset)
    print("Model fitting completed.")


def measure_metrics(model: nn.Module, dataset, metrics_config: list) -> Dict[str, float]:
    """Measure all configured metrics for the model using built-in evaluator."""
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    dataset = dataset.to(device)
    id_data = (dataset.input, dataset.output)
    ood_data = (dataset.input, dataset.output)  # For testing, data content doesn't matter
    
    try:
        evaluator = get_evaluator(metrics_config)
        results = evaluator.evaluate(model, id_data, ood_data)
        
        # Convert all values to float to ensure JSON serialization
        return {k: float(v) if v is not None else None for k, v in results.items()}
        
    except Exception as e:
        print(f"Error evaluating metrics: {e}")
        return {metric_config.get('name', 'unknown'): None for metric_config in metrics_config}


def create_experiment_result(uq_method: str, dataset_name: str, model_name: str, uq_params: Dict[str, Any], model: nn.Module, 
                           metric_results: Dict[str, float], config: Dict[str, Any]) -> ExperimentResult:
    """Create experiment result object."""
    
    total_params, trainable_params = count_model_parameters(model)
    
    # Calculate model size in MB (assuming float64)
    model_size_mb = total_params * 8 / 1024 / 1024
    
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    
    return ExperimentResult(
        uq_method=uq_method,
        parameters=uq_params,
        total_params=total_params,
        trainable_params=trainable_params,
        model_size_mb=model_size_mb,
        metric_results=metric_results,
        experiment_timestamp=timestamp,
        base_architecture=model_name,
        dataset_name=dataset_name
    )


def save_results(result: ExperimentResult, output_path: str):
    """Save experiment results to JSON file."""
    try:
        result_dict = {
            'uq_method': result.uq_method,
            'parameters': result.parameters,
            'total_params': result.total_params,
            'trainable_params': result.trainable_params,
            'model_size_mb': result.model_size_mb,
            'metric_results': result.metric_results,
            'experiment_timestamp': result.experiment_timestamp,
            'base_architecture': result.base_architecture,
            'dataset_name': result.dataset_name
        }
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(result_dict, f, indent=2)
            
        print(f"Results saved to {output_path}")
        
    except Exception as e:
        print(f"Error saving results: {e}")
        raise


def print_summary(result: ExperimentResult):
    """Print a summary of the experiment results."""
    print(f"\n{'='*60}")
    print(f"EXPERIMENT SUMMARY")
    print(f"{'='*60}")
    print(f"UQ Method: {result.uq_method}")
    print(f"Parameters: {result.parameters}")
    print(f"Total Parameters: {result.total_params:,}")
    print(f"Trainable Parameters: {result.trainable_params:,}")
    print(f"Model Size: {result.model_size_mb:.2f} MB (float64)")
    print(f"Metrics:")
    for metric_name, value in result.metric_results.items():
        if value is not None:
            if 'throughput' in metric_name.lower():
                print(f"  {metric_name}: {value:.2f} samples/sec")
            elif 'memory' in metric_name.lower():
                print(f"  {metric_name}: {value:.2f} MB")
            else:
                print(f"  {metric_name}: {value}")
        else:
            print(f"  {metric_name}: Failed to evaluate")
    print(f"Timestamp: {result.experiment_timestamp}")
    print(f"{'='*60}")


@click.command()
@click.option('--config_id', required=True, help='Configuration ID from parameter grid (e.g., no_uq_000)')
@click.option('--param_config', required=True, help='Path to parameter configuration JSON file')
@click.option('--config', default='config.yaml', help='Configuration file path')
@click.option('--output', default='result.json', help='Output file path for results')
@click.option('--dataset_name', default='five_d_uniform', help='Dataset name from config')
@click.option('--model_name', default='resnet_50_size', help='Model architecture name from config')
def main(config_id, param_config, config, output, dataset_name, model_name):
    print(f"Starting experiment for config ID: {config_id}")
    print(f"Loading parameter configuration from {param_config}")
    print(f"Loading base configuration from {config}")
    print(f"Using model architecture: {model_name}")
    
    uq_method, uq_params = load_parameter_config(param_config, config_id)
    print(f"UQ Method: {uq_method}")
    print(f"UQ Parameters: {uq_params}")
    
    config_dict = load_config(config)
    
    # Get metrics configuration
    metrics_config = config_dict.get('evaluation', {}).get('metrics', [])
    if not metrics_config:
        print("Warning: No metrics specified in configuration. Using default throughput metric.")
        metrics_config = [{'name': 'uncertainty_estimating_throughput'}]
    
    print(f"Configured metrics: {[m.get('name') for m in metrics_config]}")
    
    print(f"Building model with architecture: {model_name}")
    model = build_model(config_dict, model_name, uq_method, uq_params)
    
    print("Preparing data...")
    dataset = prepare_data(config_dict, dataset_name)
    
    fit_model(model, dataset)
    
    print("Measuring metrics...")
    metric_results = measure_metrics(model, dataset, metrics_config)
    
    result = create_experiment_result(uq_method, dataset_name, model_name, uq_params, model, metric_results, config_dict)
    
    print_summary(result)
    
    save_results(result, output)
    
    print("\nExperiment completed successfully!")



if __name__ == "__main__":
    main()